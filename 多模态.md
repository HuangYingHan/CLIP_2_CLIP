## 多模态部分

### 1. CLIP

借助自然语言监督来学习视觉



![img](https://pic1.zhimg.com/80/v2-65e1dbb935aa7804189dc100783a4940_720w.webp)

**对比学习阶段**

1. 目的：给定N个图片文本对，图片输入给Image Encoder.文本输入给Text Encoder,只有在对角线上的样本才是正样本，最尽量大化正样本的相似度，最小化负样本的相似度

2. 实现：文字和图片的编码器分别编码为向量a,向量b，如果a和b来自一对的配对，则a.b的映射应该向a靠近，如果不配对则向0靠近
3. 利用cross entropy loss的方式监督学习





### 2. LLaVA大模型

使用视觉与语言相结合的指令数据，将其转换为适当的指令格式

![img](https://pic2.zhimg.com/v2-8df5d0c2852ba56e89010c2fe91b9bb9_r.jpg)



使用CLIP视觉编码器对图像进行处理，将图像特征映射到单词嵌入空间中，使得他能与语言指令特征对齐，再传入大语言模型中训练输出语言



### 3. MiniGPT-4

类似的工作，通过提取视觉特征与先进的大型语言模型对齐



对于视觉工作，利用ViT骨干网络以及预训练的Q-Former

- Q-Former

<img src="https://pic3.zhimg.com/v2-e1c49340f38a204160835c1eb1ccf24a_r.jpg" alt="img" style="zoom:200%;" />

​		Learned-Queries通过Cross-Attention与图像特征交互，通过Self-Attention与文本的特征交互，使得这些Query能够得到两个模态的信息



第一阶段预训练

- 再大量对齐的图像文本上对模型进行预训练，以获取视觉语言知识



第二阶段：

- 使用一个较小旦高质量的图像文本数据集进行微调





### 4. BLIP

多模态的Transformer模型

- 针对问题：大多数现有的预训练模型仅在基于理解的任务或者基于生成的任务表现出色，但很少有兼顾的模型



- BLIP使用编码器解码器混合架构，它既可以作为单模态的编码器，又可以作为基于图像的文本编码器，或者基于图像的文本解码器



- 利用噪声网络数据：利用嘈杂数据训练一遍BLIP,再使用BLIP生成功能生成一系列通过预训练的Captioner生成一系列字幕，再过滤所的生成字幕，删除嘈杂字幕，得到干净的数据

![img](https://pic3.zhimg.com/v2-84e840678dcfdf82793e204f20e4fffa_r.jpg)

联合预训练目标

ITC： Image-Text Contrastive Learning- align image representation and text representation such that their mutual information is maximized. 对齐图像和文本表征，使其互信息最大化： 让正样本相似度尽量高，同时让负样本相似度尽量低



ITM： Image-Text matching- learn fine-grained alignment between image and text representation. 



LM： 大语言模型



### 5. BLIP2

将预训练好的单模态视觉模型和单模态语言模型，并将参数冻结。为了解决视觉特征的空间与文本特征的空间不容易对齐的问题，使用可训练的QFormer对齐特征，并最终将QFormer的特征输出给大语言模型

![img](https://pic2.zhimg.com/80/v2-fe33323e1deced16d57e9068e6c62c49_720w.webp)

训练第一步：联合视觉编码器训练

![img](https://pic3.zhimg.com/v2-a2a58d3db4409d4321c6d629a51725fe_r.jpg)

Q Former由两个Transformer子模块构成，两个Transformer的self attention层是共享的，即它可以同时处理图与文输入



- Image Transformer: 与图像编码器交互，用于提取视觉特征
- Text Transformer：即可充当文本编码器，也可以作为文本解码器





1. ITC： 计算 Queries 的输出 和 Text Transformer 的输出 的对比学习损失
2. ITG： 训练给定图片下，生成对应文本描述- 所训练的queries应该能提取捕获所有文本信息的视觉特征
3. ITM：MATCHING FINE TUNNING



训练第二步：联合大语言模型训练





## 大语言模型

### 1. LLaMa



架构：基于Transformer架构

1. 使用预归一化，而不是对输出进行归一化
   1. RMSNorm归一化
2. SwiGLU激活函数

```python
def swish(x, beta=1):
   return x * (1 / (1 + np.exp(-beta * x)))

def gelu(x):
   return x * norm.cdf(x)

def relu(x):
   return np.maximum(0, x)
```

​      SwiGLU是对Swish的变体，类似组一个神经网络层

```python
class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):
        super().__init__()
        hidden_dim = multiple_of * ((2 * hidden_dim // 3 + multiple_of - 1) // multiple_of)
        self.w1 = nn.Linear(dim, hidden_dim)
        self.w2 = nn.Linear(hidden_dim, dim)
        self.w3 = nn.Linear(dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))
```

3. 旋转嵌入
   1. https://zhuanlan.zhihu.com/p/642884818
   2. 绝对位置编码
      - 在计算q,k,v向量之前，计算一个位置编码向量$p_i$加到词嵌入x_i上

```python
# position 就对应 token 序列中的位置索引 i
# hidden_dim 就对应词嵌入维度大小 d
# seq_len 表示 token 序列长度
def get_position_angle_vec(position):
    return [position / np.power(10000, 2 * (hid_j // 2) / hidden_dim) for hid_j in range(hidden_dim)]

# position_angle_vecs.shape = [seq_len, hidden_dim]
position_angle_vecs = np.array([get_position_angle_vec(pos_i) for pos_i in range(seq_len)])

# 分别计算奇偶索引位置对应的 sin 和 cos 值
position_angle_vecs[:, 0::2] = np.sin(position_angle_vecs[:, 0::2])  # dim 2t
position_angle_vecs[:, 1::2] = np.cos(position_angle_vecs[:, 1::2])  # dim 2t+1

# positional_embeddings.shape = [1, seq_len, hidden_dim]
positional_embeddings = torch.FloatTensor(position_angle_vecs).unsqueeze(0)
```

旋转位置编码，需要去拟合一个位置编码方式，使其能表示query向量qm和key向量kn之间的内积，且符合
$$
<f_q(x_m, m), f_k(x_n, n)> = g(x_m, x_n, m-n)
$$


联想到二维旋转矩阵
$$
<RaX, RbY> = <X, R(b-a)Y> \\

<RaX, RbY> \\
= (RaX)^T RbY \\
= X^T Ra^T RbY \\
= X^T R(b-a) Y \\
= <X, R(b-a)Y>
$$




## 微调方法

CLIP对输入文本prompt的选择很敏感，需要仔细选择prompt模板才能表现良好



- MaPLe： 多模态prompt学习

  上下文prompt在视觉和语言分支中学习

- CLIP Adapter

​        -  添加额外线性层

​         - 通过残差连接将原始zero-shot视觉或语言嵌入到相应的网络调整特征混合







### 面试问题(oral)

1. CLIP的创新点是什么 (对比学习，学习图文匹配，达到zero shot)

   > 1. CLIP所针对的传统劳动密集型网络的问题：
   >    1. 数据集构造困难，需要大量人工标注
   >    2. 泛化性差，no seen no learning， 特定数据集只针对特定问题，任务迁移困难
   > 2. 针对上述问题，CLIP提出了一种接受自然语言监督的方式，将图像与他的文本描述直接对比（训练数据是文本与图像对），学习图像文本对的匹配关系。而在面对未见过的数据集，也只需要Text encoder学习过类似的文本描述即可较好的完成分类任务

2. CLIP的训练过程，损失函数名字，原理

   > ![img](https://pic1.zhimg.com/80/v2-65e1dbb935aa7804189dc100783a4940_720w.webp)
   > 对于一个batch中N对图像，文字对
   >
   > CLIP 采用两个模型去分别提取N个文本特征与N个图像特征
   >
   > 1. 采用Transformer网络作为encoder将 输入的文本提取出维度（N， dt)的文本特征
   > 2. 采用CNN或VIT等作为encoder等将输入的图片提取出维度为(N，di)的图像特征
   > 3. 通过projection的方式统一两个特征的维度 (源代码中将文本特征维度投射到图像特征embed_dim)
   > 4. 所得到的最终的两大特征做一个简单的点乘计算，得到一个N * N的矩阵，并乘以temperatur tau计算余弦相似度。矩阵的特征分布应符合在对角线上结果值最大，而最小化其他位置的结果. 
   >    1. 对行和列分别取softmax, 得到概率分布，再做 cross entropy loss使得概率与学习标签对应分布
   >    2. 对行和列loss取平均

3. CLIP的推理过程

   1. > 输入图片与其对应的N个猜测的text input, 并将encoder输出的image feature与text feeature做一个softmax计算预测可能性的分布

4. 中文CLIP是如何基于CLIP得到的（如何微调网络，如何微调数据集)

   > 采用了中文图对数据 (Noah Wukong数据集，zero数据集)
   >
   > 采用中文encoder (chinese roberta wwm large),并冻结VIT encoder,只对中文encoder进行训练, 

5. CLIP两大类微调方法

   <img src="https://pic1.zhimg.com/v2-65d40147884b96d0fc2838d84554a9c8_r.jpg" alt="img" style="zoom: 67%;" />

   1. CLIPA微调的网络架构

      > CLIP Adapter通过微调文本encoder以及图像encoder结构，在image encoder和text encoder的输出层添加一层线性的bottle neck,将原始的视觉或语言输入与学习特征残差链接。且在训练过程中冻结原油CLIP权重而只训练残差结构
   2. coop微调位置

      > 原本的CLIP在zero shot任务迁移时，需要生成合适的，模式固定的prompt, 甚至可以外界一个transformer做一个生成专门样式的prompt的NLP任务
      >
      > 为了避免人工设置prompt的过程，coop将text encoder中的token embeding换成了 prompt learner的输入，将prompt抽象为可学习的vector
      >![img](https://img-blog.csdnimg.cn/img_convert/3592224f110193dee026d13ffb9e5c95.png)
      
      > M代表模板长度，代表m个context token
      >
      > 两种不同的learnable prompt
      >
      > 1. Unified CONTEXT
      >    1. 不管是什么类别，learnable context 是一样的
      >       1. class 可放在句子中间或后面
      > 2. class specific context 
      >    1. 每一个类别有自己特有的context
      
      实现
   3. Maple微调位置，网络内部新增模块

      > 多模态prompt学习,与coop思想类似，但是除了语言prompt之外还引入了视觉prompt
      >
      > 1. 语言prompt与coop中的 prompt learner 类似
      > 2. 视觉prompt将ctx 共享给视觉encoder,并经过简单的线性投影后与输入图像一起输入给image encode
      >
      >    
      >
      >    多个transformer block加了learnable prompt

6. BLIP

   1. Image encoder: ViT, text encoder BERT

   2. 三个损失函数原理，具体数学形式

      > BLIP需要统一对视觉以及对应文本的理解，同时兼顾最终文字的生成，因此采用了三个损失函数，两个理解任务损失函数，一个生成任务损失函数
      >
      > 1. 对比学习目标函数 (Image-Text Contrastive Loss: ITC）
      >    1. 强调图文的同步匹配性
      >    2. 与CLIP类似，但是加了动量编码器
      >       1. sim_i2t_targets = alpha * Softmax(sim_i2t_m) + (1 - alpha) * sim_targets
      >          1. sim_i2t /sim_t2i是由encoder所学特征投影编码而来
      >          2. sim target 是典型的对角线目标矩阵
      >       2. loss_i2/t = log(softmax(sim_i2t) * simi2t_targets) ,再取平均
      >    
      > 2. 图文匹配目标函数 (Image Text Matching Loss: ITM)
      >    1. ITM是用来监督预测学习结果的准确性
      >       1. 用正负样本对比，最后做cross entropy
      >       2. 因为在训练时他会给图文的embed加上负样本，所以在创建label的时候也是按照 1，0 ，0这样子的样本去布置
      >    
      > 3. 语言模型目标函数 （Language Modeling Loss: LM）
      >    1. Bert head model的损失函数 （交叉商损失）
      >    
      >    
      >       ![img](https://pic3.zhimg.com/v2-84e840678dcfdf82793e204f20e4fffa_r.jpg)

7. BLIP2 

   >1. 相对BLIP的改进 (QFormer的架构及作用)
   >   1. 为了减少训练成本，冻结预训练参数，而因为所冻结的参数均为单模态训练而来，为了对其视觉特征空间以及文本特征空间，引入了 Q-Former从冻结的视觉编码器中提取特征，并充当视觉编码器与文本编码器之间的瓶颈
   >   2. 架构：
   >      1. 由两个Transformer组成，其中self-Attention的是共享的，同时接受Queries输入和Text输入
   >         1. Image Transformer: 输入为Learned Queries, 经过共享的Self Attention层后，在Cross attention层与Image Encoder交互
   >         2. Text Transformer： 输入为Text， 经过共享的Self Attention层输出，
   >2. 两阶段训练过程
   >   1. 阶段 1：联合视觉编码器训练
   >      1. 与BLIP相同，设置三个目标函数
   >         1. ITC：正样本相似度高，负样本相似度低
   >         2. ITG： 基于图像的文本生成，训练文本描述
   >         3. ITM: 正负样本对比
   >   2. 阶段2：联合冻结的视觉编码器于大型语言模型训练， QFormer提取出图文信息，通过投影与text tokem对齐输入大语言模型
   >
   >![img](https://pic2.zhimg.com/80/v2-fe33323e1deced16d57e9068e6c62c49_720w.webp)

8. MiniGPT4

   ![img](https://pic1.zhimg.com/80/v2-464cfeca7ef8dcb2c6e6a712321d6bb0_720w.webp)

   1. > 1. 如何通过BLIP微调而来
      >
      >    1.模型架构基于BLIP2，大预言模型采用ViCUna,只训练中间的投影层
      >
      > 2. 微调数据集是如何构建的
      >
      >    1. 第一阶段数据集：
      >       1. 基于第一预训练阶段的模型，设计遵循ViCUNA对话格式的prompt, 使模型对图片给出描述，并通过控制生成句子达到一定的token数保证得到的图片描述足够丰富
      >       2. 随机选择5000个图像，采用上述方法得到的详细描述信息，此时得到的描述是有许多噪声的，通过chatgot进行语法，删重等过滤。并最终手动验证筛选出3500张高质量的信息
      >    2. 第二阶段数据集：
      >       1. 采用固定的prompt，用上述高质量数据lamaForQuestionAnsweri集fine tunning网络

      

9. LLAVA
   1. LLava与BLIP2的区别
   2. QFormer是如何被替代的

      > 与BLIP系列相比缺少了图文特征对齐的过程，而是由Vision Enoder提取出特征后，线性投影（轻量化）与指令特征对齐输入到大语言模型中

10. LLAMA
    1. LLAMA网络架构及设计原理

       > LLAMA的网络架构基于Transformer修改而来，仍然由Attention和embedding堆叠而成
       >
       > 但是：
       >
       > 1. 采用在从embeded patch提取QKV前使用RMSNorm （和layer norm相比省略了减去均值的部分)
       >
       >    ![image-20240225230358461](/home/yinghanhuang/snap/typora/86/.config/Typora/typora-user-images/image-20240225230358461.png)
       >
       > 2. 在QK上采用旋转位置编码  (数学形式)
       >
       >    1. 位置编码与query或key相乘， Rm表示第m个位置的位置编码，而右侧qi表示对应位置的query响亮
       >       1. ![img](https://pic1.zhimg.com/80/v2-befa6058db4177c657538775446ebfc8_720w.webp)
       >
       > 3. 使用Casual mask保证每个位置只能看到前向的tocken
       >
       > 4. SwagRelu （数学描述）
       >
       >    1. ```pyt
       >       def gelu(x):
       >          return x * norm.cdf(x)
       >                                                                                     
       >       def relu(x):
       >          return np.maximum(0, x)
       >                                                                                     
       >       def swish(x, beta=1):
       >          return x * (1 / (1 + np.exp(-beta * x)))
       >       ```

    2. 训练策略

    3. 损失函数形式

       > Cross Entropy
       >
       > 通过比较tokens于目标tokens的相似度得到,比如next token prediction

11. Vicuna
    1. 如何基于LLAMA微调而来

       > 用监督数据微调而来，以70k条对话（来源）为输入，输出由GPT4和真实用户来打分



12 . self.attention 和cross attention diff

> 主要区别在计算注意力分数时，所用的q,k,v来源不同
>
> - self attention用于计算输入序列中每个元素之间的关系
>   - 输入序列被分别卷积为三个向量（q查询向量，k键向量，v值向量），用于计算每个输入元素之间的注意力分数。主要用于计算同一输入的上下文关系
> - cross attention用于计算两个不同序列中元素之间的关系
>   - cross attention有两个不同的输入序列，其中一个序列被用作查询序列，另一个用作键和值向量，cross attention计算的是第一个序列中每个元素与第二个序列中所有元素之间的注意力关系。



13. Multihead attention原理和作用

> 多头将embed vector映射后的q,k,v分割，平行地从计算从输入中选取多个信息
>
> MultiHead(Q, K, V) = Concat(head, head) W^0
>
> ​	where head_i = Attention(QW_i^Q， KW_i^K, VW_i^V)
>
>
> 作用：
>
> 1. 扩展了模型专注于不同位置的能力
>
> 2. 为注意力提供了多个自空间，有多组Q，K，V权重矩阵，将embed_bed投影到不同的子空间中，多个头学习的侧重点可能略有不同，这样使得模型拥有更大的容量
>
>    
>
>    ![Scaled Dot-Product Attention VS Multi-Head Attention](https://oss.imzhanghao.com/img/202109151148991.png)

14. 手写 multihead attention

> class dot_product_attention
>
> ​	def forward(q,k,v):
>
> ​		attention = torch.bmm(q, k, transpose(1, 2)) 
>
> ​		attention = self.softmax(attention)
>
> ​		context = torch.bmm(attention, v)
>
> ​		return context, attention		

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, model_dim = 400, num_head = 4, dropout = 0.0):
        self().__init__()
        
        self.dim_per_head = model_dim // num_head
        self.num_head = num_head
        self.linear_k = nn.Linear(model_dim, self.dim_per_head * self.num_head)
        self.linear_v = nn.Linear(model_dim, self.dim_per_head * self.num_head)
        self.linear_q = nn.Linear(model_dim, self.dim_per_head * self.num_head)
        
        self.linear_out = self.nn.Linear(model_dim, model_dim)
        self.dropout = self.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(model_dim)
    def forward(self, key, value, query, attn_mask=None):
        
        batch_size = key.size()
        
        key = self.linear_k(key)
        value = self.linear_v(value)
        query = self.linear_q(query)
        
        key = key.view(batch_size * self.num_head, -1, dim_per_head)
        value = value.view(batch_size * self.num_head, -1, dim_per_head)
        query = query.view(batch_size * self.num_head, -1, dim_per_head)
        
        context, attention = self.dot_product_attention(query, key, value)
        
        context = context.view(batchsize, -1, dim_per_head * self.num_head)
        output = self.linear_out(context)
        output = self.layer_norm(output)
        
        return output, atttention
        
        
```





15. Gemini 论文阅读 (论文跟进)

> Gemini的模型架构主要基于Transformer的decoder，采用了高效的注意力机制，如多查询注意力机制等，使其能够在大规模GPU上进行稳定训练，并优化在单张设备的推理性能
>
> Encoder:
>
> - 视觉编码主要是基于谷歌先前的Flamimgo, CoCa等
> - 视频编码：将视频作为一系列帧编码并处理可变的输入分辨率
> - 语言编码：使用USM模型处理16khz的音频信号
> - 训练方法 - 

16. 多模态领域的看法

- 学术界： 解决更多问题， 端到端高精度解决问题
- 业务：图像文本匹配（目前更多
  - 多模态解决CV更多问题 ，为CV提供一个的统一解决方案

17. BEIT 微软 v1 - v3改进过程

> BEIT v1解决的问题：
>
>  1. 使用dVAE将图像Patch编码为视觉标志(Visual token)
>
>     	1. Auto Encoder结构， encoder提取出visual token（将图像抽象为信息密集的载体）, docoder重建图像
>         	1. ![image-20240225231653816](/home/yinghanhuang/snap/typora/86/.config/Typora/typora-user-images/image-20240225231653816.png)
>         	2. x 输入图像， z是图像编码的特征向量，q_phi为visual token的分布,p_是一个均匀分布
>             	1. 第一部分只将视觉标志z还原为输入图像的只来那个，第二部分表征输入输出图像的分布一致性
>
>  2. 使用BERT架构和Mask Image Model结合预测图像掩码部分预测的视觉标示
>
>     1. MLM是对掩码单词进行预测填充
>
>        	1. 将N个patches随机生成M个掩码
>
>        ![img](https://pic4.zhimg.com/v2-14ecf3db24c190ab0759289656b5a6f3_r.jpg)
>
>     2. BERT输入为图像的所有patch，预测的是Dvae生活层的视觉标志
>
>        	1. 进行掩码操作后输入transformer,对比预测的视觉标志和dVAE的视觉标志的交叉熵损失
>        		1. ![image-20240225232749354](/home/yinghanhuang/snap/typora/86/.config/Typora/typora-user-images/image-20240225232749354.png)

> BEIT V2:
>
> 	1. V1对视觉标志没有很好的优化和探讨
> 	1. V2通过训练好的模型如clip等作为teacher指导视觉标志学习 (vector-quantized knowledge distillation)。同时引入cls标识符学习整个图像的特征

> BEIT V3:
>
> 40层Multiway transformers![img](https://pic4.zhimg.com/80/v2-438ad80fb2880dad1dca22c6f58ac0bb_720w.webp)
>
> 用同一个共享的多头注意力模块处理输入的图像，文本以及图像文本对,在分别接入不同模态的前馈网络（三个模块的损失，学习过程）

18. 原始attention is all your need -- decoder mask attention实现

> self-attention层允许一个位置的token对每个位置的token产生影响，但在实际过程中应该保持注意力集中在当前以及之前时刻的输入，而非后来的输入，因此应该在softmax之前给注意力权重矩阵乘以一个注意力掩码（NLP人物）

![img](https://pic4.zhimg.com/v2-9eff2ab05d1aec1aa0aae907423c037b_r.jpg)





19. 大语言模型 SFT有监督微调，RLHF，COT
20. Chatglm版本 v1-v4
21. CLIP AD和maple





# Test Result

### CLIP

### 1. test

5个来自5个正样本类别的图片和对应6个text (5个positive + others)

| Precision | Recall |
| --------- | ------ |
| 0.5797    | 0.8328 |

1个来自1个类别的正样本图片其他的others类

| data                | Precision | Recall |
| ------------------- | --------- | ------ |
| cat                 | 0.6938    | 0.9141 |
| dog                 | 0.7221    | 0.9170 |
| gecko               | 0.6624    | 0.8839 |
| caoshu              | 0.5843    | 0.7318 |
| sweet and sour pork | 0.6391    | 0.8590 |



一个类别的图片和对应text(similarity >= 0.2)

| dataset | Recall | Precision |
| ------- | ------ | --------- |
| cat     | 0.915  | 0.802     |
| dog     | 0.9707 | 0.573     |
| Gecko   | 0.925  | 0.559     |
| cao shu | 0.8458 | 0.482     |
| meat    | 0.995  | 0.501     |

(similarity >= 0.25)

| dataset | Recall | Precision |
| ------- | ------ | --------- |
| cat     | 0.653  | 0.997     |
| dog     | 0.715  | 0.701     |
| Gecko   | 0.51   | 0.990     |
| cao shu | 0.01   | 0.333     |
| meat    | 0.69   | 0.624     |

### 2. English clip and Chinese clip

taiyi clip

| dataset | Precision | Recall |
| ------- | --------- | ------ |
| cat     | 0.996     | 0.996  |
| dog     | 0.777     | 0.722  |
| gecko   | 0.849     | 0.875  |
| cao shu | 0.912     | 0.909  |
| meat    | 0.819     | 0.776  |
| average | 0.871     | 0.856  |

English Clip

| dataset | Precision | Recall |
| ------- | --------- | ------ |
| cat     | 0.994     | 0.994  |
| dog     | 0.955     | 0.953  |
| gecko   | 0.895     | 0.895  |
| cao shu | 0.526     | 0.527  |
| meat    | 0.620     | 0.584  |
| average | 0.798     | 0.791  |

并集准确率：

|       | precision | recall |
| ----- | --------- | ------ |
| taiyi | 0.738     | 0.867  |
| clip  | 0.693     | 0.836  |



经过LLAVA pick后的

|       | precision | recall |
| ----- | --------- | ------ |
| taiyi | 0.852     | 0.913  |
| clip  | 0.834     | 0.901  |



图搜图试验结果(English clip)

只用正样本：

| dataset | Recall |
| ------- | ------ |
| cat     | 0.931  |
| dog     | 0.975  |
| gecko   | 0.941  |
| cao shu | 0.926  |
| meat    | 0.920  |
|         |        |

正负样本：

| dataset | Precision | Recall |
| ------- | --------- | ------ |
| cat     | 0.915     | 0.924  |
| dog     | 0.748     | 0.876  |
| gecko   | 0.827     | 0.901  |
| cao shu | 0.260     | 0.710  |
| meat    | 0.720     | 0.875  |